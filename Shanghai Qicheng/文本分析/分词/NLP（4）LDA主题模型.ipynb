{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 苏伦NLP第四课"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  一、前言\n",
    "    前三课，我们学习了nlp的总体概述、文本分词jieba的使用，还学习了关键词抽取、热词抽取，然后学习了词向量表示和word2vec，那么这一课我们来学习篇章级别的主题分析。\n",
    "\n",
    "## 二、LDA 简介\n",
    "    首先，我们来感受下LDA是什么，\n",
    "    LDA（Latent Dirichlet Allocation）是一种文档主题生成模型，也称为一个三层贝叶斯概率模型，包含词、主题和文档三层结构。\n",
    "    所谓生成模型，就是说，我们认为一篇文章的每个词都是通过“以一定概率选择了某个主题，并从这个主题中以一定概率选择某个词语”这样一个过程得到。\n",
    "    文档到主题服从多项式分布，主题到词服从多项式分布。\n",
    "\n",
    "    *看到这里我们只需要先记住：LDA的目的就是要识别主题，即把文档—词汇矩阵变成文档—主题矩阵（分布）和主题—词汇矩阵（分布）\n",
    "\n",
    "## 三、LDA模型构建过程\n",
    "### 3.1 LDA生成流程\n",
    "    对于语料库中的每篇文档，LDA定义了如下生成过程（generativeprocess）：\n",
    "    1.对每一篇文档，从主题分布中抽取一个主题；\n",
    "    2.从上述被抽到的主题所对应的单词分布中抽取一个单词；\n",
    "    3.重复上述过程直至遍历文档中的每一个单词。\n",
    "\n",
    "    语料库中的每一篇文档与T（通过反复试验等方法事先给定）个主题的一个多项分布 （multinomialdistribution）相对应，将该多项分布记为θ。每个主题又与词汇表（vocabulary）中的V个单词的一个多项分布相对应，将这个多项分布记为φ。\n",
    "\n",
    "\n",
    "\n",
    "### 3.2 LDA 整体流程\n",
    "#### 3.2.1 相关定义\n",
    "    先定义一些字母的含义：文档集合D，主题（topic)集合T\n",
    "    D中每个文档d看作一个单词序列<w1,w2,...,wn>，wi表示第i个单词，设d有n个单词。（LDA里面称之为wordbag，实际上每个单词的出现位置对LDA算法无影响）\n",
    "\n",
    "    D中涉及的所有不同单词组成一个大集合VOCABULARY（简称VOC），LDA以文档集合D作为输入，希望训练出的两个结果向量（设聚成k个topic，VOC中共包含m个词）\n",
    "    对每个D中的文档d，对应到不同Topic的概率θd<pt1,...,ptk>，其中，pti表示d对应T中第i个topic的概率。计算方法是直观的，pti=nti/n，其中nti表示d中对应第i个topic的词的数目，n是d中所有词的总数。\n",
    "    对每个T中的topict，生成不同单词的概率φt<pw1,...,pwm>，其中，pwi表示t生成VOC中第i个单词的概率。计算方法同样很直观，pwi=Nwi/N，其中Nwi表示对应到topict的VOC中第i个单词的数目，N表示所有对应到topic的单词总数。\n",
    "    LDA的核心公式如下：\n",
    "    p(w|d)=p(w|t)*p(t|d)\n",
    "    直观的看这个公式，就是以Topic作为中间层，可以通过当前的θd和φt给出了文档d中出现单词w的概率。其中p(t|d)利用θd计算得到，p(w|t)利用φt计算得到。\n",
    "    实际上，利用当前的θd和φt，我们可以为一个文档中的一个单词计算它对应任意一个Topic时的p(w|d)，然后根据这些结果来更新这个词应该对应的topic。然后，如果这个更新改变了这个单词所对应的Topic，就会反过来影响θd和φt。\n",
    "\n",
    "#### 3.2.2 学习过程\n",
    "    LDA算法开始时，先随机地给θd和φt赋值（对所有的d和t）。然后上述过程不断重复，最终收敛到的结果就是LDA的输出。再详细说一下这个迭代的学习过程：\n",
    "    1.针对一个特定的文档ds中的第i单词wi，如果令该单词对应的topic为tj，可以把上述公式改写为：\n",
    "    pj(wi|ds)=p(wi|tj)*p(tj|ds)\n",
    "    2.现在我们可以枚举T中的topic，得到所有的pj(wi|ds)，其中j取值1~k。然后可以根据这些概率值结果为ds中的第i个单词wi选择一个topic。最简单的想法是取令pj(wi|ds)最大的tj（注意，这个式子里只有j是变量），即argmax[j]pj(wi|ds)\n",
    "    3.然后，如果ds中的第i个单词wi在这里选择了一个与原先不同的topic，就会对θd和φt有影响了（根据前面提到过的这两个向量的计算公式可以很容易知道）。它们的影响又会反过来影响对上面提到的p(w|d)的计算。对D中所有的d中的所有w进行一次p(w|d)的计算并重新选择topic看作一次迭代。这样进行n次循环迭代之后，就会收敛到LDA所需要的结果了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 四、实例\n",
    "### 4.1 计算文档-词汇矩阵\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    w1 w2 ... wn  \n",
    "D1  0  2      5  \n",
    "D2  3  1      6  \n",
    "...  \n",
    "Dn  2  4      2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 计算主题-词汇矩阵"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    w1 w2 ... wn  \n",
    "Z1  8  10     11  \n",
    "Z2  6  12     10  \n",
    "...  \n",
    "Zn  7  13     12  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 计算文档主题矩阵\n",
    "#### 统计每个词代表的主题在每一个文档中出现的次数，可得出以下矩阵文档—主题矩阵"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    Z1 Z2 ... Zn  \n",
    "D1  2  3      5  \n",
    "D2  3  6      3  \n",
    "...  \n",
    "Dn  1  3      5  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 五、LDA 文档生成流程\n",
    "### 5.1 LDA 假设文档是由多个主题的混合来产生的，每个文档的生成过程如下：\n",
    "\n",
    "    从全局的泊松分布参数为 β 的分布中生成一个文档的长度 N\n",
    "    从全局的狄利克雷参数为 alpha 的分布中生成一个当前文档的 θ\n",
    "    对当前文档长度 N 的每一个字都有\n",
    "    从 θ 为参数的多项式分布生成一个主题的下标 zn\n",
    "    从 θ 和 z 共同为参数的多项式分布中，产生一个字 wn\n",
    "    这些主题基于词的概率分布来产生词，给定文档数据集，LDA 可以学习出，是哪些主题产生了这些文档。\n",
    "\n",
    "    对于文档生成过程，则有，首先对于文档 n 中的每一个字，都先从文档矩阵 M1 中的 θi 中产生一个下标，告诉我们现在要从主题矩阵 M2 中的哪一行 ϕm生成当前的字。\n",
    "\n",
    "\n",
    "\n",
    "### 5.2训练过程（吉布斯采样）\n",
    "    吉布斯采样 (Gibbs Sampling) 首先选取概率向量的一个维度，给定其他维度的变量值当前维度的值，不断收敛来输出待估计的参数。具体地\n",
    "\n",
    "    随机给每一篇文档的每一个词 w，随机分配主题编号 z\n",
    "    统计每个主题 zi 下出现字 w 的数量，以及每个文档 n 中出现主题 zi 中的词 w的数量\n",
    "    每次排除当前词 w 的主题分布 zi，根据其他所有词的主题分类，来估计当前词 w 分配到各个主题 z1,z2,…,zk 的概率，即计算 p(zi|z−i,d,w) (Gibbs updating rule))。得到当前词属于所有主题z1,z2,…,zk 的概率分布后，重新为词采样一个新的主题 z1。用同样的方法不断更新的下一个词的主题，直到每个文档下的主题分布θn 和每个主题下的词分布 ϕk 收敛。\n",
    "    最后输出待估计参数，θn 和 ϕk ，每个单词的主题 zn,k 也可以得到。\n",
    "\n",
    "\n",
    "    LDA 对于每个文档的每一个字都有一个主题下标。但从文档聚类的角度来说，LDA 没有一个文档统一的聚类标签，而是每个字都有一个聚类标签，这个就是主题。LDA 每个字都有可能属于不同的类别，每个文档都有可能属于不同的类别。在大量的迭代后，主题分布和字分布都比较稳定也比较好了，LDA 模型收敛。\n",
    "\n",
    "### 5.3LDA 的参数\n",
    "    α ：表示 document-topic 密度， α 越高，文档包含的主题更多，反之包含的主题更少\n",
    "\n",
    "    β ：表示 topic-word 密度， β 越高，主题包含的单词更多，反之包含的单词更少\n",
    "\n",
    "    主题数量：主题数量从语料中抽取得到，使用 Kullback Leibler Divergence Score 可以获取最好的主题数量。\n",
    "\n",
    "    主题词数：组成一个主题所需要的词的数量。这些词的数量通常根据需求得到，如果说需求是抽取特征或者关键词，那么主题词数比较少，如果是抽取概念或者论点，那么主题词数比较多。\n",
    "\n",
    "    迭代次数：使得 LDA 算法收敛的最大迭代次数\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 六、Running in Python\n",
    "#### 准备文档集合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = \"Sugar is bad to consume. My sister likes to have sugar, but not my father.\"\n",
    "doc2 = \"My father spends a lot of time driving my sister around to dance practice.\"\n",
    "doc3 = \"Doctors suggest that driving may cause increased stress and blood pressure.\"\n",
    "doc4 = \"Sometimes I feel pressure to perform well at school, but my father never seems to drive my sister to do better.\"\n",
    "doc5 = \"Health experts say that Sugar is not good for your lifestyle.\"\n",
    "\n",
    "# 整合文档数据\n",
    "doc_complete = [doc1, doc2, doc3, doc4, doc5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据清洗和预处理\n",
    "#数据清洗对于任何文本挖掘任务来说都非常重要，在这个任务中，移除标点符号，停用词和标准化语料库（Lemmatizer，对于英文，将词归元）。\n",
    "from nltk import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "doc_clean = [clean(doc).split() for doc in doc_complete]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备 Document - Term 矩阵\n",
    "# 语料是由所有的文档组成的，要运行数学模型，将语料转化为矩阵来表达是比较好的方式。LDA 模型在整个 DT 矩阵中寻找重复的词语模式。\n",
    "# Python 提供了许多很好的库来进行文本挖掘任务，“genism” 是处理文本数据比较好的库。下面的代码掩饰如何转换语料为 Document - Term 矩阵：\n",
    "\n",
    "import genism\n",
    "from gensim import corpora\n",
    "\n",
    "# 创建语料的词语词典，每个单独的词语都会被赋予一个索引\n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "\n",
    "# 使用上面的词典，将转换文档列表（语料）变成 DT 矩阵\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建 LDA 模型\n",
    "# 创建一个 LDA 对象，使用 DT 矩阵进行训练。训练需要上面的一些超参数，gensim 模块允许 LDA 模型从训练语料中进行估计，\n",
    "# 并且从新的文档中获得对主题分布的推断。\n",
    "\n",
    "# 使用 gensim 来创建 LDA 模型对象\n",
    "Lda = genism.models.ldamodel.LdaModel\n",
    "\n",
    "# 在 DT 矩阵上运行和训练 LDA 模型\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出结果\n",
    "print(ldamodel.print_topics(num_topics=3, num_words=3))\n",
    "\n",
    "[\n",
    "    '0.168*health + 0.083*sugar + 0.072*bad,\n",
    "    '0.061*consume + 0.050*drive + 0.050*sister,\n",
    "    '0.049*pressur + 0.049*father + 0.049*sister\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 每一行包含了主题词和主题词的权重，Topic 1 可以看作为“不良健康习惯”，Topic 3 可以看作 “家庭”。\n",
    "\n",
    "## 七、提高主题模型结果的一些方法\n",
    "#### 主题模型的结果完全取决于特征在语料库中的表示，但是语料通常表示为比较稀疏的文档矩阵，因此减少矩阵的维度可以提升主题模型的结果。\n",
    "\n",
    "#### 1. 根据词频调整稀疏矩阵\n",
    "    根据频率来分布词，高频词更可能出现在结果中，低频词实际上是语料库中的弱特征，对于词频进行分析，可以决定什么频率的值应该被视为阈值。\n",
    "\n",
    "#### 2. 根据词性标注 (Part of Speech Tag) 调整稀疏矩阵\n",
    "    比起频率特征，词性特征更关注于上下文的信息。主题模型尝试去映射相近的词作为主题，但是每个词在上下文上有可能重要性不同，比如说介词 “IN” 包含 “within”，“upon”, “except”，基数词 “CD” 包含：许多(many)，若干（several)，个把(a，few)等等，情态助动词 “MD” 包含 “may”，“must” 等等，这些词可能只是语言的支撑词，对实际意义影响不大，因此可以通过词性来消除这些词的影响。\n",
    "#### 3. 调整 LDA 的 Batch 大小\n",
    "    为了得到主题中最重要的主题词，语料可以被分为固定大小的 batch，在这些 batch 上运行 LDA 模型会提供不同的结果，但是最佳的主题词会在这些 batch 上有交集。\n",
    "\n",
    "\n",
    "\n",
    "#### **主题模型用于特征选择\n",
    "    比如说文本分类任务中，LDA 可以用来选择特征，因为训练数据中含有类别信息，可以在不同类别的结果中，删除相同的、比较常见的主题词，为主题类别提供更好的特征。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
