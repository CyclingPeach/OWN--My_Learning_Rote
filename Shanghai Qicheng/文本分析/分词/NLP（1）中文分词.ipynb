{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实验介绍\n",
    "1. 实验内容\n",
    "      \n",
    "\n",
    "    本节主要阐述了NLP的技术点，从分析对象、内容两个角度分两块来介绍，将带你了解nlp有哪些基础任务。\n",
    "   \n",
    "  \n",
    " \n",
    "2. 实验目标\n",
    "\n",
    "\n",
    "    能对本NLP课程有一个初步的把握，掌握jieba分词的三个特点（支持 3 种分词模式、支持繁体分词 、支持自定义词典）的使用方法。\n",
    "\n",
    "\n",
    "3. 实验环境\n",
    "\n",
    "\n",
    "      Python 3.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实验过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 分词\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jieba.cut(text,cut_all=True)  \n",
    "分词函数，第一个参数是需要分词的字符串，第二个参数表示是否为全模式。分词返回的结果是一个可迭代的生成器（generator），可使用for循环来获取分词后的每个词语，更推荐读者转换为list列表再使用。\n",
    "\n",
    "\n",
    "jieba.cut_for_search(text)  \n",
    "搜索引擎模式分词，参数为分词的字符串，该方法适合用于搜索引擎构造倒排索引的分词，粒度比较细。\n",
    "\n",
    "#### # 尽量不要使用 GBK 字符串，可能无法预料地错误解码成 UTF-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入 jieba\n",
    "import jieba\n",
    "import jieba.posseg as pseg #词性标注\n",
    "import jieba.analyse as anls #关键词提取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 全模式、精确模式、搜索引擎模式\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"小杨毕业于北京理工大学，从事Python人工智能相关工作。\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'小/杨/毕业/于/北京理工大学/，/从事/Python/人工智能/相关/工作/。'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 【自己测试】\n",
    "data = jieba.cut(text)\n",
    "data\n",
    "'/'.join(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Building prefix dict from the default dictionary ...\n",
    "    Dumping model to file cache C:\\Users\\m1595\\AppData\\Local\\Temp\\jieba.cache\n",
    "    Loading model cost 0.699 seconds.\n",
    "    Prefix dict has been built successfully.\n",
    "\n",
    "    从默认字典构建前缀字典...\n",
    "    将模型转储到文件缓存 C:\\Users\\m1595\\AppData\\Local\\Temp\\jieba.cache\n",
    "    加载模型花费 0.699 秒。\n",
    "    前缀 dict 已成功构建。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**全模式**    **`jieba.cut(text, cut_all=True)`**\n",
    "\n",
    "**精确模式**    **`jieba.cut(text)`**\n",
    "\n",
    "**搜索引擎模式**    **`jieba.cut_for_search(text)`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'generator'>\n",
      "【全模式】:  小/杨/毕业/于/北京/北京理工/北京理工大学/理工/理工大/理工大学/工大/大学/，/从事/Python/人工/人工智能/智能/相关/工作/。\n"
     ]
    }
   ],
   "source": [
    " # 全模式\n",
    "data = jieba.cut(text,cut_all=True)\n",
    "print(type(data))\n",
    "print(u\"【全模式】: \", \"/\".join(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【精确模式】:  小/杨/毕业/于/北京理工大学/，/从事/Python/人工智能/相关/工作/。\n"
     ]
    }
   ],
   "source": [
    "# 精确模式  \n",
    "data = jieba.cut(text,cut_all=False)\n",
    "print(u\"【精确模式】: \", \"/\".join(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[默认模式]:  小杨/毕业/于/北京理工大学/，/从事/Python/人工智能/相关/工作/。\n"
     ]
    }
   ],
   "source": [
    "# 默认是精确模式 \n",
    "data = jieba.cut(text)  \n",
    "print(u\"【默认模式】: \", \"/\".join(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[搜索引擎模式]:  小杨/毕业/于/北京/理工/工大/大学/理工大/北京理工大学/，/从事/Python/人工/智能/人工智能/相关/工作/。\n"
     ]
    }
   ],
   "source": [
    "# 搜索引擎模式\n",
    "data = jieba.cut_for_search(text)\n",
    "print(u\"【搜索引擎模式】: \", \"/\".join(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 HMM 模型——隐马尔可夫模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "在Jieba工具中，对于未登录到词库的词，可以使用了基于汉字成词能力的 `HMM 模型`和 `Viterbi` 算法，其大致原理是采用四个隐含状态，分别表示为单字成词、词组的开头、词组的中间和词组的结尾。通过标注好的分词训练集，可以得到 `HMM` 的各个参数，然后使用 `Viterbi` 算法来解释测试集，得到分词结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【未启用HMM模型】:  他/来到/了/网易/杭/研/大厦/工作/，/我/继续/去/北/理/读研/。\n",
      "【启用 HMM 模型】:  他/来到/了/网易/杭研/大厦/工作/，/我/继续/去/北理/读研/。\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "  \n",
    "text = \"他来到了网易杭研大厦工作，我继续去北理读研。\"\n",
    "# 精确模式\n",
    "data = jieba.cut(text, cut_all=False, HMM=False)\n",
    "print(u\"【未启用HMM模型】: \", \"/\".join(data))\n",
    "# 精确模式+HMM\n",
    "data = jieba.cut(text, cut_all=False, HMM=True)\n",
    "print(u\"【启用 HMM 模型】: \", \"/\".join(data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结果所示，未启用HMM模型时，它无法识别“杭研”、“北理”词语，将其拆分成了“杭”、“研”和“北”、“理”，而启用HMM模型时，它有效识别了新词“杭研”、“北理”。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 繁体字分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 繁体字文本\n",
    "ft_text = \"\"\"人生易老天難老 歲歲重陽 今又重陽 戰地黃花分外香 壹年壹度秋風勁 不似春光 勝似春光 寥廓江天萬裏霜 \"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【全模式】：人生/ 易/ 老天/ 難/ 老/ /  / / 歲/ 歲/ 重/ 陽/ /  / / 今/ 又/ 重/ 陽/ /  / / 戰/ 地/ 黃/ 花/ 分外/ 香/ /  / / 壹年/ 壹/ 度/ 秋/ 風/ 勁/ /  / / 不似/ 春光/ /  / / 勝/ 似/ 春光/ /  / / 寥廓/ 江天/ 萬/ 裏/ 霜/ /  / \n"
     ]
    }
   ],
   "source": [
    "# 全模式\n",
    "print(\"【全模式】：\" + \"/ \".join(jieba.cut(ft_text, cut_all=True))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【精确模式】：人生/ 易/ 老天/ 難老/  / 歲/ 歲/ 重陽/  / 今/ 又/ 重陽/  / 戰地/ 黃/ 花/ 分外/ 香/  / 壹年/ 壹度/ 秋風勁/  / 不/ 似/ 春光/  / 勝似/ 春光/  / 寥廓/ 江天/ 萬/ 裏/ 霜/  \n"
     ]
    }
   ],
   "source": [
    "# 精确模式\n",
    "print(\"【精确模式】：\" + \"/ \".join(jieba.cut(ft_text, cut_all=False)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【搜索引擎模式】：人生/ 易/ 老天/ 難老/  / 歲/ 歲/ 重陽/  / 今/ 又/ 重陽/  / 戰地/ 黃/ 花/ 分外/ 香/  / 壹年/ 壹度/ 秋風勁/  / 不/ 似/ 春光/  / 勝似/ 春光/  / 寥廓/ 江天/ 萬/ 裏/ 霜/  \n"
     ]
    }
   ],
   "source": [
    "# 搜索引擎模式\n",
    "print(\"【搜索引擎模式】：\" + \"/ \".join(jieba.cut_for_search(ft_text)))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 添加自定义词典"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "虽然Jieba工具有新词识别能力，但也无法识别出所有Jieba词库里没有的词语，但它为开发者提供了添加自定义词典功能，从而保证更好的分词正确率。\n",
    "\n",
    "\n",
    "其函数原型如下：  \n",
    "**`load_userdict(f)`**该函数只有一个参数，表示载入的自定义词典路径。\n",
    "\n",
    "```Python\n",
    "jieba.load_userdict(\"dict.txt\")\n",
    "```\n",
    "\n",
    "词典的格式为：一个词占一行，每行分为三部分：\n",
    "\n",
    "**`word freq word_type`**\n",
    "\n",
    "**`word`**为对应的词语，**`freq`**为词频（可省略），**`word_type`**为词性（可省略），中间用空格隔开，顺序不可以改变。\n",
    "\n",
    "注意，文件必须为**UTF-8编码**。虽然 jieba 有新词识别能力，但自行添加新词可以保证更高的正确率。\n",
    "\n",
    "例如：  \n",
    "```Python\n",
    "创新办 3 i  \n",
    "云计算 5  \n",
    "凱特琳 nz\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 载入词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示例文本\n",
    "text = \"小杨在贵州财经大学工作，擅长大数据、云计算，喜欢乾清宫、黄果树瀑布等景区。\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[原始文本]:  小杨在贵州财经大学工作，擅长大数据、云计算，喜欢乾清宫、黄果树瀑布等景区。\n",
      "[精确模式]:  小/杨/在/贵州/财经大学/工作/，/擅长/大/数据/、/云/计算/，/喜欢/乾/清宫/、/黄果树/瀑布/等/景区/。\n"
     ]
    }
   ],
   "source": [
    "# 未加载词典\n",
    "# 精确模式  \n",
    "data = jieba.cut(text, cut_all=False)\n",
    "print(u\"[原始文本]: \", text)\n",
    "print(u\"[精确模式]: \", \"/\".join(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[原始文本]:  小杨在贵州财经大学工作，擅长大数据、云计算，喜欢乾清宫、黄果树瀑布等景区。\n",
      "[精确模式]:  小/杨/在/贵州财经大学/工作/，/擅长/大/数据/、/云计算/，/喜欢/乾清宫/、/黄果树/瀑布/等/景区/。\n"
     ]
    }
   ],
   "source": [
    "# 载入词典\n",
    "text = \"小杨在贵州财经大学工作，擅长大数据、云计算，喜欢乾清宫、黄果树瀑布等景区。\"  \n",
    "\n",
    "#导入自定义词典\n",
    "jieba.load_userdict(\"dict.txt\")\n",
    "#精确模式\n",
    "data = jieba.cut(text, cut_all=False)\n",
    "print(u\"[原始文本]: \", text)\n",
    "print(u\"[精确模式]: \", \"/\".join(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此时的输出结果有效地提取了“贵州财经大学”、“云计算”、“乾清宫”。但也有两个未识别出的词语，“黄果树瀑布”不在词典中，故被拆分成了“黄果树”和“瀑布”，“大数据”虽然在词典中，却仍然拆分成了“大”和“数据”。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 动态修改词典"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在`jieba`工具中，可以在程序中**动态修改词典**\n",
    "\n",
    "通过**`add_word(word, freq=None, tag=None)`**函数添加新词语\n",
    "\n",
    "通过**`del_word(word)`**函数删除自定义词语。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[原始文本]:  小杨在贵州财经大学工作，擅长大数据、云计算，喜欢乾清宫、黄果树瀑布等景区。 \n",
      "\n",
      "[精确模式]:  小杨/在/贵州财经大学/工作/，/擅长/大/数据/、/云计算/，/喜欢/乾清宫/、/黄果树瀑布/等/景区/。\n"
     ]
    }
   ],
   "source": [
    "text = \"小杨在贵州财经大学工作，擅长大数据、云计算，喜欢乾清宫、黄果树瀑布等景区。\"  \n",
    "\n",
    "#导入自定义词典\n",
    "jieba.load_userdict(\"dict.txt\")\n",
    "\n",
    "#添加自定义词语\n",
    "jieba.add_word(\"小杨\")\n",
    "jieba.add_word(\"黄果树瀑布\")\n",
    "jieba.add_word(\"自然语言处理\", freq=10, tag=\"nz\")\n",
    "\n",
    "#删除自定义词语\n",
    "jieba.del_word(\"北理工\")\n",
    "\n",
    "#精确模式  \n",
    "data = jieba.cut(text, cut_all=False)\n",
    "print(u\"[原始文本]: \", text, \"\\n\")\n",
    "print(u\"[精确模式]: \", \"/\".join(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为什么“大数据”被拆分成了“大”和“数据”呢？这是因为Jieba词库中“大”和“数据”的重要程度更高\n",
    "\n",
    "我们可以使用**`suggest_freq(segment, tune=True)`**函数调节单个词语的词频，使其被分割出来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 调节词频\n",
    "jieba.suggest_freq('大数据', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[原始文本]:  小杨在贵州财经大学工作，擅长大数据、云计算，喜欢乾清宫、黄果树瀑布等景区。 \n",
      "\n",
      "[精确模式]:  小杨/在/贵州财经大学/工作/，/擅长/大数据/、/云计算/，/喜欢/乾清宫/、/黄果树瀑布/等/景区/。\n"
     ]
    }
   ],
   "source": [
    "# 调节词频后\n",
    "# 精确模式  \n",
    "data = jieba.cut(text, cut_all=False)\n",
    "print(u\"[原始文本]: \", text, \"\\n\")\n",
    "print(u\"[精确模式]: \", \"/\".join(data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
