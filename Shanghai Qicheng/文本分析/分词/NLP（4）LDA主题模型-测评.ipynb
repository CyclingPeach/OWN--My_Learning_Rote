{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一、在线测评"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请根据以下语料构建一个LDA模型\n",
    "'''\n",
    "doc1 = \"Text classification is a classic topic for natural language processing, in which one needs to assign predefined categories to free-text documents.\"\n",
    "\n",
    "doc2 = \"Our models accept a sequence of encoded characters as input. The encoding is done by prescribing an alphabet of size m for the input language, and then quantize each character using 1-of-m encoding (or “one-hot” encoding).\"\n",
    "\n",
    "doc3 = \"Previous research on ConvNets in different areas has shown that they usually work well with largescale datasets, especially when the model takes in low-level raw features like characters in our case. \"\n",
    "\n",
    "doc4 = \"We gratefully acknowledge the support of NVIDIA Corporation with the donation of 2 Tesla K40 GPUs used for this research. We gratefully acknowledge the support of Amazon.com Inc for an AWS in Education Research grant used for this research.\"\n",
    "\n",
    "doc5 = \"Figure 3f shows that changing the alphabet by distinguishing between uppercase and lowercase letters could make a difference.\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二、在线测评答案[待定]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "见：【解答过程】"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 三、解答过程[不严谨]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = \"Text classification is a classic topic for natural language processing, in which one needs to assign predefined categories to free-text documents.\"\n",
    "\n",
    "doc2 = \"Our models accept a sequence of encoded characters as input. The encoding is done by prescribing an alphabet of size m for the input language, and then quantize each character using 1-of-m encoding (or “one-hot” encoding).\"\n",
    "\n",
    "doc3 = \"Previous research on ConvNets in different areas has shown that they usually work well with largescale datasets, especially when the model takes in low-level raw features like characters in our case. \"\n",
    "\n",
    "doc4 = \"We gratefully acknowledge the support of NVIDIA Corporation with the donation of 2 Tesla K40 GPUs used for this research. We gratefully acknowledge the support of Amazon.com Inc for an AWS in Education Research grant used for this research.\"\n",
    "\n",
    "doc5 = \"Figure 3f shows that changing the alphabet by distinguishing between uppercase and lowercase letters could make a difference.\"\n",
    "\n",
    "# 整合文档数据\n",
    "doc_complete = [doc1, doc2, doc3, doc4, doc5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text classification is a classic topic for natural language processing, in which one needs to assign predefined categories to free-text documents. \n",
      "\n",
      "Our models accept a sequence of encoded characters as input. The encoding is done by prescribing an alphabet of size m for the input language, and then quantize each character using 1-of-m encoding (or “one-hot” encoding). \n",
      "\n",
      "Previous research on ConvNets in different areas has shown that they usually work well with largescale datasets, especially when the model takes in low-level raw features like characters in our case.  \n",
      "\n",
      "We gratefully acknowledge the support of NVIDIA Corporation with the donation of 2 Tesla K40 GPUs used for this research. We gratefully acknowledge the support of Amazon.com Inc for an AWS in Education Research grant used for this research. \n",
      "\n",
      "Figure 3f shows that changing the alphabet by distinguishing between uppercase and lowercase letters could make a difference. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in doc_complete:\n",
    "    print(i, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\TaoZhiyuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\TaoZhiyuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['text',\n",
       "  'classification',\n",
       "  'classic',\n",
       "  'topic',\n",
       "  'natural',\n",
       "  'language',\n",
       "  'processing',\n",
       "  'one',\n",
       "  'need',\n",
       "  'assign',\n",
       "  'predefined',\n",
       "  'category',\n",
       "  'freetext',\n",
       "  'document'],\n",
       " ['model',\n",
       "  'accept',\n",
       "  'sequence',\n",
       "  'encoded',\n",
       "  'character',\n",
       "  'input',\n",
       "  'encoding',\n",
       "  'done',\n",
       "  'prescribing',\n",
       "  'alphabet',\n",
       "  'size',\n",
       "  'input',\n",
       "  'language',\n",
       "  'quantize',\n",
       "  'character',\n",
       "  'using',\n",
       "  '1ofm',\n",
       "  'encoding',\n",
       "  'or',\n",
       "  '“onehot”',\n",
       "  'encoding'],\n",
       " ['previous',\n",
       "  'research',\n",
       "  'convnets',\n",
       "  'different',\n",
       "  'area',\n",
       "  'shown',\n",
       "  'usually',\n",
       "  'work',\n",
       "  'well',\n",
       "  'largescale',\n",
       "  'datasets',\n",
       "  'especially',\n",
       "  'model',\n",
       "  'take',\n",
       "  'lowlevel',\n",
       "  'raw',\n",
       "  'feature',\n",
       "  'like',\n",
       "  'character',\n",
       "  'case'],\n",
       " ['gratefully',\n",
       "  'acknowledge',\n",
       "  'support',\n",
       "  'nvidia',\n",
       "  'corporation',\n",
       "  'donation',\n",
       "  '2',\n",
       "  'tesla',\n",
       "  'k40',\n",
       "  'gpus',\n",
       "  'used',\n",
       "  'research',\n",
       "  'gratefully',\n",
       "  'acknowledge',\n",
       "  'support',\n",
       "  'amazoncom',\n",
       "  'inc',\n",
       "  'aws',\n",
       "  'education',\n",
       "  'research',\n",
       "  'grant',\n",
       "  'used',\n",
       "  'research'],\n",
       " ['figure',\n",
       "  '3f',\n",
       "  'show',\n",
       "  'changing',\n",
       "  'alphabet',\n",
       "  'distinguishing',\n",
       "  'uppercase',\n",
       "  'lowercase',\n",
       "  'letter',\n",
       "  'could',\n",
       "  'make',\n",
       "  'difference']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 数据清洗和预处理\n",
    "'''\n",
    "    数据清洗对于任何文本挖掘任务来说都非常重要，在这个任务中，\n",
    "    移除标点符号、停用词和标准化语料库（Lemmatizer，对于英文，将词归元）\n",
    "'''\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "stop = set(stopwords.words('english'))    # 停用词\n",
    "exclude = set(string.punctuation)    # 标点符号\n",
    "lemma = WordNetLemmatizer()    # \n",
    "\n",
    "# 定义一个函数，用于去停用词、去标点符号 和 标准化语料库(即词形还原)\n",
    "def clean(doc):\n",
    "    stop_free = ' '.join([i for i in doc.lower().split() if i not in stop])    # 去停用词\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)    # 去标点符号\n",
    "    normalized = ' '.join(lemma.lemmatize(word) for word in punc_free.split())    # 词形还原\n",
    "    return normalized\n",
    "\n",
    "# 调用函数\n",
    "doc_clean = [clean(doc).split() for doc in doc_complete]\n",
    "\n",
    "# 查看数据清洗和预处理后的文档\n",
    "doc_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['text',\n",
       "  'classification',\n",
       "  'classic',\n",
       "  'topic',\n",
       "  'natural',\n",
       "  'language',\n",
       "  'processing',\n",
       "  'one',\n",
       "  'need',\n",
       "  'assign',\n",
       "  'predefined',\n",
       "  'category',\n",
       "  'freetext',\n",
       "  'document'],\n",
       " ['model',\n",
       "  'accept',\n",
       "  'sequence',\n",
       "  'encoded',\n",
       "  'character',\n",
       "  'input',\n",
       "  'encoding',\n",
       "  'done',\n",
       "  'prescribing',\n",
       "  'alphabet',\n",
       "  'size',\n",
       "  'input',\n",
       "  'language',\n",
       "  'quantize',\n",
       "  'character',\n",
       "  'using',\n",
       "  '1ofm',\n",
       "  'encoding',\n",
       "  'or',\n",
       "  '“onehot”',\n",
       "  'encoding'],\n",
       " ['previous',\n",
       "  'research',\n",
       "  'convnets',\n",
       "  'different',\n",
       "  'area',\n",
       "  'shown',\n",
       "  'usually',\n",
       "  'work',\n",
       "  'well',\n",
       "  'largescale',\n",
       "  'datasets',\n",
       "  'especially',\n",
       "  'model',\n",
       "  'take',\n",
       "  'lowlevel',\n",
       "  'raw',\n",
       "  'feature',\n",
       "  'like',\n",
       "  'character',\n",
       "  'case'],\n",
       " ['gratefully',\n",
       "  'acknowledge',\n",
       "  'support',\n",
       "  'nvidia',\n",
       "  'corporation',\n",
       "  'donation',\n",
       "  '2',\n",
       "  'tesla',\n",
       "  'k40',\n",
       "  'gpus',\n",
       "  'used',\n",
       "  'research',\n",
       "  'gratefully',\n",
       "  'acknowledge',\n",
       "  'support',\n",
       "  'amazoncom',\n",
       "  'inc',\n",
       "  'aws',\n",
       "  'education',\n",
       "  'research',\n",
       "  'grant',\n",
       "  'used',\n",
       "  'research'],\n",
       " ['figure',\n",
       "  '3f',\n",
       "  'show',\n",
       "  'changing',\n",
       "  'alphabet',\n",
       "  'distinguishing',\n",
       "  'uppercase',\n",
       "  'lowercase',\n",
       "  'letter',\n",
       "  'could',\n",
       "  'make',\n",
       "  'difference']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    数据清洗对于任何文本挖掘任务来说都非常重要，在这个任务中，\n",
    "    移除标点符号、停用词和标准化语料库（Lemmatizer，对于英文，将词归元）\n",
    "'''\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "stop = set(stopwords.words('english'))    # 停用词\n",
    "exclude = set(string.punctuation)    # 标点符号\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "# 定义一个函数，用于去停用词、去标点符号 和 标准化语料库(即词形还原)\n",
    "def clean(doc):\n",
    "    stop_free = ' '.join([i for i in doc.lower().split() if i not in stop])    # 去停用词\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)    # 去标点符号\n",
    "    normalized = ' '.join(lemma.lemmatize(word) for word in punc_free.split())    # 词形还原\n",
    "    return normalized\n",
    "\n",
    "# 调用函数\n",
    "doc_clean = [clean(doc).split() for doc in doc_complete]\n",
    "\n",
    "# 查看数据清洗和预处理后的文档\n",
    "doc_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1),\n",
       "  (1, 1),\n",
       "  (2, 1),\n",
       "  (3, 1),\n",
       "  (4, 1),\n",
       "  (5, 1),\n",
       "  (6, 1),\n",
       "  (7, 1),\n",
       "  (8, 1),\n",
       "  (9, 1),\n",
       "  (10, 1),\n",
       "  (11, 1),\n",
       "  (12, 1),\n",
       "  (13, 1)],\n",
       " [(6, 1),\n",
       "  (14, 1),\n",
       "  (15, 1),\n",
       "  (16, 1),\n",
       "  (17, 2),\n",
       "  (18, 1),\n",
       "  (19, 1),\n",
       "  (20, 3),\n",
       "  (21, 2),\n",
       "  (22, 1),\n",
       "  (23, 1),\n",
       "  (24, 1),\n",
       "  (25, 1),\n",
       "  (26, 1),\n",
       "  (27, 1),\n",
       "  (28, 1),\n",
       "  (29, 1)],\n",
       " [(17, 1),\n",
       "  (22, 1),\n",
       "  (30, 1),\n",
       "  (31, 1),\n",
       "  (32, 1),\n",
       "  (33, 1),\n",
       "  (34, 1),\n",
       "  (35, 1),\n",
       "  (36, 1),\n",
       "  (37, 1),\n",
       "  (38, 1),\n",
       "  (39, 1),\n",
       "  (40, 1),\n",
       "  (41, 1),\n",
       "  (42, 1),\n",
       "  (43, 1),\n",
       "  (44, 1),\n",
       "  (45, 1),\n",
       "  (46, 1),\n",
       "  (47, 1)],\n",
       " [(42, 3),\n",
       "  (48, 1),\n",
       "  (49, 2),\n",
       "  (50, 1),\n",
       "  (51, 1),\n",
       "  (52, 1),\n",
       "  (53, 1),\n",
       "  (54, 1),\n",
       "  (55, 1),\n",
       "  (56, 1),\n",
       "  (57, 2),\n",
       "  (58, 1),\n",
       "  (59, 1),\n",
       "  (60, 1),\n",
       "  (61, 2),\n",
       "  (62, 1),\n",
       "  (63, 2)],\n",
       " [(16, 1),\n",
       "  (64, 1),\n",
       "  (65, 1),\n",
       "  (66, 1),\n",
       "  (67, 1),\n",
       "  (68, 1),\n",
       "  (69, 1),\n",
       "  (70, 1),\n",
       "  (71, 1),\n",
       "  (72, 1),\n",
       "  (73, 1),\n",
       "  (74, 1)]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    准备 文档——主题（Document - Term） 矩阵，即 D-T 矩阵\n",
    "    语料是由所有的文档组成的，要运行数学模型，将语料转化为矩阵来表达是比较好的方式。\n",
    "    LDA 模型在整个 D-T 矩阵中寻找重复的词语模式。\n",
    "    Python提供了许多很好的库来进行文本挖掘任务，‘gensim’库是处理文本数据比较好的库。\n",
    "    下面的代码掩饰如何转换语料为 D-T 矩阵：\n",
    "'''\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "# 创建语料的词语词典，每个单独的词语都会被赋予一个索引\n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "\n",
    "# 使用上面的词典，将转换文档列表（语料）变成 D-T 矩阵\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "\n",
    "# 查看 D-T 矩阵\n",
    "doc_term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    创建一个 LDA 对象，使用 D-T  矩阵进行训练。\n",
    "    训练需要上面的一些超参数，gensim 模块允许 LDA 模型从训练语料中进行估计，\n",
    "    并且从新的文档中获得对主题分布的推断。\n",
    "'''\n",
    "# 使用 gensim 来创建 LDA 模型对象\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# 在 D-T 矩阵上运行和训练 LDA 模型\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '0.023*\"model\" + 0.023*\"previous\" + 0.023*\"lowlevel\"'),\n",
       " (1, '0.034*\"language\" + 0.034*\"processing\" + 0.034*\"predefined\"'),\n",
       " (2, '0.048*\"research\" + 0.048*\"encoding\" + 0.034*\"character\"')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 输出结果\n",
    "ldamodel.print_topics(num_topics=3, num_words=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
