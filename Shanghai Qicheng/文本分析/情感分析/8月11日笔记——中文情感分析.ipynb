{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 中文情感分析\n",
    "### 步骤\n",
    "    1、读取评论数据集\n",
    "    2、数据预处理(去空、去重)\n",
    "    3、机械压缩\n",
    "    4、短句过滤\n",
    "    5、snownlp库——打分\n",
    "    6、打分后分词\n",
    "    7、gensim库——分析\n",
    "\n",
    "### 知识点\n",
    "    1、pandas读取数据\n",
    "    2、空值删除\n",
    "    3、去重复值（两个）\n",
    "    4、apply函数、lambda函数\n",
    "    5、机械压缩去词\n",
    "    6、短句过滤\n",
    "    7、snownlp库——进行情感分析【打分】\n",
    "    8、打分后分词\n",
    "    9、读取停用词文件\n",
    "    10、去停用词\n",
    "    11、gensim库——进行情感分析\n",
    "### 不懂的地方\n",
    "    1、两个函数(apply函数、lambda函数)\n",
    "    2、机械压缩的含义/解释\n",
    "    3、情感分析的两个库【打分库，分析库】\n",
    "    4、分析后的数值\n",
    "    5、分词后的数据为什么被分成几份了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 笔记\n",
    "\n",
    "[删除某列中含有空值的行][1]\n",
    "\n",
    "[1]:https://blog.csdn.net/weixin_43474731/article/details/100155593"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>score</th>\n",
       "      <th>disliked</th>\n",
       "      <th>likes</th>\n",
       "      <th>liked</th>\n",
       "      <th>ctime</th>\n",
       "      <th>content</th>\n",
       "      <th>last_ep_index</th>\n",
       "      <th>cursor</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>緖山まひろ</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>9981</td>\n",
       "      <td>0</td>\n",
       "      <td>1530982477</td>\n",
       "      <td>我有个想法，动物体内的细胞会不会是兽耳呢d(ŐдŐ๑)</td>\n",
       "      <td>0</td>\n",
       "      <td>77395314810369</td>\n",
       "      <td>2018-07-07 16:54:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>onlylove丶亚丝娜</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1557139155</td>\n",
       "      <td>题材新颖</td>\n",
       "      <td>0</td>\n",
       "      <td>77395314810369</td>\n",
       "      <td>2019-05-06 10:39:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AKAHANE_铃木羽</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1556947242</td>\n",
       "      <td>坐等第二季！！表白白细胞∠( ᐛ 」∠)＿</td>\n",
       "      <td>0</td>\n",
       "      <td>77395314810369</td>\n",
       "      <td>2019-05-04 05:20:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>妖怪退散</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1557182714</td>\n",
       "      <td>好看，推荐</td>\n",
       "      <td>0</td>\n",
       "      <td>77395314810369</td>\n",
       "      <td>2019-05-06 22:45:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>受虎大王</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1557118709</td>\n",
       "      <td>涨知识</td>\n",
       "      <td>0</td>\n",
       "      <td>77395314810369</td>\n",
       "      <td>2019-05-06 04:58:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19058</th>\n",
       "      <td>八奇大蛇</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1533969368</td>\n",
       "      <td>不配</td>\n",
       "      <td>0</td>\n",
       "      <td>74186973181452</td>\n",
       "      <td>2018-08-11 06:36:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19059</th>\n",
       "      <td>我不听我就不改名</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1533473635</td>\n",
       "      <td>第一次对一个网站以及一部番恶心到这种地步</td>\n",
       "      <td>0</td>\n",
       "      <td>74186973181452</td>\n",
       "      <td>2018-08-05 12:53:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19060</th>\n",
       "      <td>截教至圣通天教主</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1530984474</td>\n",
       "      <td>还可以</td>\n",
       "      <td>0</td>\n",
       "      <td>71618582349408</td>\n",
       "      <td>2018-07-07 17:27:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19061</th>\n",
       "      <td>小寒、很忙</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1530983251</td>\n",
       "      <td>血小板太萌了</td>\n",
       "      <td>0</td>\n",
       "      <td>71618582349408</td>\n",
       "      <td>2018-07-07 17:07:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19062</th>\n",
       "      <td>水無月珀尔MinazukiPER</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1530982189</td>\n",
       "      <td>先给个保底，期待接下来的表现</td>\n",
       "      <td>0</td>\n",
       "      <td>71618582349408</td>\n",
       "      <td>2018-07-07 16:49:49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19063 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 author  score  disliked  likes  liked       ctime  \\\n",
       "0                 緖山まひろ     10         0   9981      0  1530982477   \n",
       "1          onlylove丶亚丝娜     10         0      1      0  1557139155   \n",
       "2           AKAHANE_铃木羽     10         0      3      0  1556947242   \n",
       "3                  妖怪退散     10         0      0      0  1557182714   \n",
       "4                  受虎大王     10         0      0      0  1557118709   \n",
       "...                 ...    ...       ...    ...    ...         ...   \n",
       "19058              八奇大蛇      2         0      1      0  1533969368   \n",
       "19059          我不听我就不改名      2         0      0      0  1533473635   \n",
       "19060          截教至圣通天教主      2         0      0      0  1530984474   \n",
       "19061             小寒、很忙      8         0      0      0  1530983251   \n",
       "19062  水無月珀尔MinazukiPER      4         0      0      0  1530982189   \n",
       "\n",
       "                           content last_ep_index          cursor  \\\n",
       "0      我有个想法，动物体内的细胞会不会是兽耳呢d(ŐдŐ๑)             0  77395314810369   \n",
       "1                             题材新颖             0  77395314810369   \n",
       "2            坐等第二季！！表白白细胞∠( ᐛ 」∠)＿             0  77395314810369   \n",
       "3                            好看，推荐             0  77395314810369   \n",
       "4                              涨知识             0  77395314810369   \n",
       "...                            ...           ...             ...   \n",
       "19058                           不配             0  74186973181452   \n",
       "19059         第一次对一个网站以及一部番恶心到这种地步             0  74186973181452   \n",
       "19060                          还可以             0  71618582349408   \n",
       "19061                       血小板太萌了             0  71618582349408   \n",
       "19062               先给个保底，期待接下来的表现             0  71618582349408   \n",
       "\n",
       "                      date  \n",
       "0      2018-07-07 16:54:37  \n",
       "1      2019-05-06 10:39:15  \n",
       "2      2019-05-04 05:20:42  \n",
       "3      2019-05-06 22:45:14  \n",
       "4      2019-05-06 04:58:29  \n",
       "...                    ...  \n",
       "19058  2018-08-11 06:36:08  \n",
       "19059  2018-08-05 12:53:55  \n",
       "19060  2018-07-07 17:27:54  \n",
       "19061  2018-07-07 17:07:31  \n",
       "19062  2018-07-07 16:49:49  \n",
       "\n",
       "[19063 rows x 10 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r'F:\\陶志远\\数据\\数据集\\B站流行动漫影评数据\\B站流行动漫影评数据.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''删除不需要的数据'''\n",
    "\n",
    "del_col = ['disliked','likes','liked','ctime','last_ep_index','cursor','date']   # 要删除的列\n",
    "df = df.drop(del_col, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>score</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>緖山まひろ</td>\n",
       "      <td>10</td>\n",
       "      <td>我有个想法，动物体内的细胞会不会是兽耳呢d(ŐдŐ๑)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>onlylove丶亚丝娜</td>\n",
       "      <td>10</td>\n",
       "      <td>题材新颖</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AKAHANE_铃木羽</td>\n",
       "      <td>10</td>\n",
       "      <td>坐等第二季！！表白白细胞∠( ᐛ 」∠)＿</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>妖怪退散</td>\n",
       "      <td>10</td>\n",
       "      <td>好看，推荐</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>受虎大王</td>\n",
       "      <td>10</td>\n",
       "      <td>涨知识</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19058</th>\n",
       "      <td>八奇大蛇</td>\n",
       "      <td>2</td>\n",
       "      <td>不配</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19059</th>\n",
       "      <td>我不听我就不改名</td>\n",
       "      <td>2</td>\n",
       "      <td>第一次对一个网站以及一部番恶心到这种地步</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19060</th>\n",
       "      <td>截教至圣通天教主</td>\n",
       "      <td>2</td>\n",
       "      <td>还可以</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19061</th>\n",
       "      <td>小寒、很忙</td>\n",
       "      <td>8</td>\n",
       "      <td>血小板太萌了</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19062</th>\n",
       "      <td>水無月珀尔MinazukiPER</td>\n",
       "      <td>4</td>\n",
       "      <td>先给个保底，期待接下来的表现</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19063 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 author  score                      content\n",
       "0                 緖山まひろ     10  我有个想法，动物体内的细胞会不会是兽耳呢d(ŐдŐ๑)\n",
       "1          onlylove丶亚丝娜     10                         题材新颖\n",
       "2           AKAHANE_铃木羽     10        坐等第二季！！表白白细胞∠( ᐛ 」∠)＿\n",
       "3                  妖怪退散     10                        好看，推荐\n",
       "4                  受虎大王     10                          涨知识\n",
       "...                 ...    ...                          ...\n",
       "19058              八奇大蛇      2                           不配\n",
       "19059          我不听我就不改名      2         第一次对一个网站以及一部番恶心到这种地步\n",
       "19060          截教至圣通天教主      2                          还可以\n",
       "19061             小寒、很忙      8                       血小板太萌了\n",
       "19062  水無月珀尔MinazukiPER      4               先给个保底，期待接下来的表现\n",
       "\n",
       "[19063 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df.dropna()\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "去重数据后：\n",
      "19063\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0\n",
       "0  10\n",
       "1   8\n",
       "2   6\n",
       "3   2\n",
       "4   4"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 去掉第一列的重复数据；iloc[:, 0]表示索引每一行的第一列；\n",
    "df3 = pd.DataFrame(df2.iloc[:, 1].unique()) \n",
    "print('去重数据后：')\n",
    "print(len(df))\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#首先定义一个函数，去除重复数据。\n",
    "def str_unique(raw_str, reverse=False):\n",
    "    \"\"\"\n",
    "    比如：我喜欢喜欢喜欢喜欢喜欢喜欢该商品；去掉重复的“喜欢”\n",
    "    :param raw_str:\n",
    "    :param reverse: 是否转置\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if reverse:\n",
    "        raw_str = raw_str[::-1]\n",
    "    res_str = ''\n",
    "    for i in raw_str:\n",
    "        if i not in res_str:\n",
    "            res_str += i\n",
    "    if reverse:\n",
    "        res_str = res_str[::-1]\n",
    "    return res_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-9d60dfdcee6c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#使用 apply 方法应用函数；\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mser1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr_unique\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 这时，因为索引了第一列，所以结果成了 Series；\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'df2'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mser1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# <class 'pandas.core.series.Series'>\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdf4\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mser1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr_unique\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 再次生成 DataFrame；\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'机械压缩去词后：'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Software\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   3846\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3847\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3848\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3850\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-4418b0a7a552>\u001b[0m in \u001b[0;36mstr_unique\u001b[1;34m(raw_str, reverse)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mraw_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mraw_str\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mres_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_str\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mres_str\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[0mres_str\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "#使用 apply 方法应用函数；\n",
    "ser1 = df3.iloc[:, 0].apply(str_unique) # 这时，因为索引了第一列，所以结果成了 Series；\n",
    "print('df2', type(ser1)) # <class 'pandas.core.series.Series'>\n",
    "df4 = pd.DataFrame(ser1.apply(str_unique, reverse=True)) # 再次生成 DataFrame；\n",
    "print('机械压缩去词后：')\n",
    "print(len(df4))\n",
    "print(type(df4))\n",
    "print('------------------')\n",
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = df4[df4.iloc[:, 0].apply(len) >= 4]\n",
    "print('短句过滤后：')\n",
    "print(len(df5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snownlp import SnowNLP\n",
    "coms = df5.iloc[:, 0].apply(lambda x: SnowNLP(x).sentiments)\n",
    "print('情感分析后')\n",
    "positive_df = df5[coms >= 0.9]\n",
    "negative_df = df5[coms <= 0.1]\n",
    "print('特别喜欢的')\n",
    "print(positive_df)\n",
    "print('___________________________')\n",
    "print('不喜欢的')\n",
    "print(negative_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "my_cut = lambda s:' '.join(jieba.cut(s))\n",
    "positive_ser = positive_df.iloc[:, 0].apply(my_cut)\n",
    "negative_ser = negative_df.iloc[:, 0].apply(my_cut)\n",
    "print('大于0.9——正面数据——分词')\n",
    "print(positive_ser)\n",
    "print('小于0.1——负面数据——分词')\n",
    "print(negative_ser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = pd.read_csv(r'E:\\Jupyter Notebook\\常用表格文件\\漂流瓶\\stoplist2.txt', encoding='UTF-8', header=None, sep='tipdm', engine='python')\n",
    "stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = [' ', ' '] +list(stops[0])\n",
    "positive_df = pd.DataFrame(positive_ser)\n",
    "negative_df = pd.DataFrame(negative_ser)\n",
    "positive_df[1] = positive_df[0].apply(lambda s: s.split(' '))\n",
    "positive_df[2] = positive_df[1].apply(lambda x: [i for i in x if i.encode('UTF-8') not in stops])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 这里的```lambda```函数实在不懂"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=============================================================')\n",
    "print('去停用词后：positive_df')\n",
    "print(positive_df)\n",
    "print('=============================================================')\n",
    "print('去停用词后：negative_df')\n",
    "print(negative_df)\n",
    "print('=============================================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gensim库——进行主题分析，分析出人们对商品及其属性的情感倾向。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "# 正面主题分析\n",
    "pos_dict = corpora.Dictionary(positive_df[2])    # 建立词典\n",
    "pos_corpus = [pos_dict.doc2bow(i) for i in positive_df[2]]    # 建立语料库\n",
    "pos_lda = models.LdaModel(pos_corpus, num_topics=3, id2word = pos_dict)    #LDA 模型训练\n",
    "print('正面主题分析')\n",
    "for i in range(3):\n",
    "    print('topic', i)\n",
    "    print(pos_lda.print_topic(i))    # 输出每个主题\n",
    "\n",
    "# 负面主题分析\n",
    "neg_dict = corpora.Dictionary(negative_df[2])  # 建立词典\n",
    "neg_corpus = [neg_dict.doc2bow(i) for i in negative_df[2]]    # 建立语料库\n",
    "neg_lda = models.LdaModel(neg_corpus, num_topics=3, id2word = neg_dict)    #LDA 模型训练\n",
    "print('负面主题分析')\n",
    "for i in range(3):\n",
    "    print('topic', i)\n",
    "    print(neg_lda.print_topic(i))    # 输出每个主题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(r'F:\\陶志远\\同步文件夹\\数据集\\漂流瓶\\comments.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 缺失值删除\n",
    "[使用pandas中的 dropna 方法过滤缺失数据][1]\n",
    "\n",
    "[Pandas详解十之Dropna滤除缺失数据][2]\n",
    "\n",
    "[1]:https://blog.csdn.net/weixin_44706915/article/details/102688651?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-3.nonecase&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-3.nonecase\n",
    "[2]:https://blog.csdn.net/weixin_38168620/article/details/79596798"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### dropna() 函数\n",
    "```df.dropna()```\n",
    "\n",
    "默认删除行\n",
    "\n",
    "参数:axis \\ how \\ thresh \\ subset \\ inplace\n",
    "\n",
    "    axis = 0  表示删除行\n",
    "    axis = 1  表示删除列\n",
    "    \n",
    "    how = 'any'  表示存在NAN就删除\n",
    "    how = 'all'  表示全部为NAN才删除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#生成一个4行5列的DataFrame数据，注意列名的写法\n",
    "df = pd.DataFrame(np.random.randn(4,5), columns=list('abcde'))\n",
    "df.loc[1,['b', 'd']] = np.nan    #生成NAN\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#默认删除所有含有NAN的行\n",
    "df1 = df.dropna()\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#删除所有含有NAN的行\n",
    "#axis = 0\n",
    "df2 = df.dropna(axis = 0) \n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#删除所有含有NAN的列\n",
    "#axis = 1\n",
    "df3 = df.dropna(axis=1)\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#增加一列全部为NAN\n",
    "df4 = df\n",
    "df4['f'] = np.nan\n",
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how='any'\n",
    "# 只要含有NAN就删除\n",
    "df5 = df4.dropna(axis=0, how='any') # 搜索含有NAN的行，然后删除行\n",
    "df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how='any'\n",
    "# 只要含有NAN就删除\n",
    "df6 = df4.dropna(axis=1, how='any') # 搜索含有NAN的列，然后删除列\n",
    "df6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how='all'\n",
    "# 每行都含NAN的列\n",
    "df7 = df4.dropna(axis=1, how='all')\n",
    "df7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤\n",
    "    1、读取评论数据集\n",
    "    2、数据预处理(去空、去重)\n",
    "    3、机械压缩\n",
    "    4、短句过滤\n",
    "    5、snownlp库——打分\n",
    "    6、打分后分词\n",
    "    7、gensim库——分析\n",
    "\n",
    "### 知识点\n",
    "    1、pandas读取数据\n",
    "    2、空值删除\n",
    "    3、去重复值（两个）\n",
    "    4、apply函数、lambda函数\n",
    "    5、机械压缩去词\n",
    "    6、短句过滤\n",
    "    7、snownlp库——进行情感分析【打分】\n",
    "    8、打分后分词\n",
    "    9、读取停用词文件\n",
    "    10、去停用词\n",
    "    11、gensim库——进行情感分析\n",
    "### 不懂的地方\n",
    "    1、两个函数(apply函数、lambda函数)\n",
    "    2、机械压缩的含义/解释\n",
    "    3、情感分析的两个库【打分库，分析库】\n",
    "    4、分析后的数值\n",
    "    5、分词后的数据为什么被分成几份了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 去重复值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#生成一个4行5列的DataFrame数据，注意列名的写法\n",
    "af = pd.DataFrame([[2,3,'你'],[2,4,6],['a','t','y'],['你','在','你']], columns=list('beg'))\n",
    "af"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "af2 = af.iloc[:, 2].unique()\n",
    "pd.DataFrame(af2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
